\documentclass[a4paper,12pt]{article}
\usepackage{titling}
\usepackage[OT4]{fontenc}
\usepackage[english,polish]{babel}
\usepackage{amsmath, amsfonts, amsthm, latexsym}
\DeclareRobustCommand{\sqbullet}{%
	\raisebox{.3ex}{\rule{.3em}{.3em}}%
}
% Margins in document
%\usepackage[left=2.5cm, right=2.5cm, top=3.5cm]{geometry}

% Indentation at the beginning of chapters/sections
\usepackage{indentfirst}

% Ceiling functions
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% Avoid  colons before tables' empty captions and change caption
\usepackage{caption}
\captionsetup[table]{name=Tab.}
\captionsetup[figure]{name=Rys.}

% Don't know why, it starts from 2
\addtocounter{table}{0}

% Rename tables' suffix
\renewcommand{\tablename}{Tab.}

% Graphicx setup
\usepackage{graphicx}
\graphicspath{{graphics/}{../graphics/}}

% No separator between items
\usepackage{enumitem}
\setlist{nolistsep}

% Pagebreak before every \section
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

% Bigger padding in tabulars
\usepackage{array}
\setlength\extrarowheight{3pt}

%\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

% Itemize in tabulars (avoid big margins with minipage)
\newcommand{\tabbeditemize}[1]{
	\begin{minipage}[t]{0.4\textwidth}
		\begin{itemize}[topsep=0mm,partopsep=0mm,leftmargin=4mm]
			#1
		\end{itemize}
\end{minipage}}

% listings
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{pdfpages}
\definecolor{codegreen}{rgb}{0,0.7,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
	language=Python,
	deletekeywords={from},
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=4
}
\makeatletter
\newcommand*{\toccontents}{\@starttoc{tableofcontents}}
\makeatother


% DOCUMENT
\title{Sprawozdanie \\
\Large Zastosowanie uczenia maszynowego do gry wyścigowej}

\author{Mateusz Burczaniuk,\\ Patryk Fijałkowski}
\date{07.12.2020}


% ============================================
% CONTENT ====================================
% ============================================

\begin{document}
	\maketitle
	%\vspace{3cm}
	\tableofcontents

\section{Wprowadzenie}
\subsection{Problem}
Omawiana gra wyścigowa symuluje rywalizację na torze pomiędzy kilkoma samochodami wyścigowymi. Tor, na którym odbywa się rywalizacja jest zamkniętą pętlą otoczoną niezbyt wysokimi ścianami. Poza tym na torze występują także przeszkody. Samochód może poruszać się do przodu, do tyłu, skręcać w~lewo, prawo lub wykorzystać nitro. Zderzenie samochodu gracza ze ścianą lub przeszkodą powoduje natychmiastową śmierć gracza i~koniec gry. Zderzenie dwóch graczy ze sobą powoduje analogiczne skutki (śmierć i~usunięcie z~rozgrywki) dla obu graczy.

Gracz ma zadanie przejechać zadaną trasę jak najszybciej, wyprzedzając swoich rywali. Podczas rywalizacji gracz może znacznie zwiększyć swoją prędkość przy użyciu nitro. Jednak maksymalna dostępna w~danej chwili ilość nitro jest ograniczona, więc gracz nie może używać go bez przerwy. Zużyte nitro stopniowo odnawia się i~po pewnym czasie wraca do pierwotnego poziomu. Może ono odnawiać się nieskończoną liczbę razy.

Wszystkie rozważane pojazdy mają taki same parametry techniczne i~osiągi, w~związku z~czym wygrana lub przegrana w~wyścigu zależy wyłącznie od obranego przez nie sposobu przemieszczania się po torze.

Zatem postawiony problem składa się z~kilku zagadnień - należy zoptymalizować politykę doboru kierunku poruszania się gracza oraz używania nitro tak, aby przejechać całą trasę bez żadnego zderzenia, a~jednocześnie jak najszybciej, pozostawiając rywali za sobą. Zadania te w~pewnym stopniu konfliktują ze sobą, gdyż optymalną strategią w~celu uniknięcia zderzeń jest stanie w~miejscu, natomiast dążenie do uzyskania optymalnego czasu przejazdu powoduje wzrost ryzyka zderzenia z~przeszkodą lub innym pojazdem.

%Cel projektu

\subsection{Stan wiedzy}

Omawiany problem reprezentuje zagadnienie kontroli w~ciągłej przestrzeni stanów. Istnieje kilka sposobów rozwiązywania tego typu problemów. Do czołowych algorytmów w~tej dziedzinie należą obecnie zarówno metody on-policy, jak i~off policy. W~niniejszej pracy w~kontekście omawianego zadania przetestowano dwie stosunkowo młode metody, ale znacznie różniące się metody: PPO oraz SAC.

%Krótkie omówienie „stanu techniki”–czy jest jakiś benchmark do porównania?To może być 1-2 zdania

\subsection{PPO}
Proximal Policy Optimization, czyli PPO to nowa metoda, wprowadzona w~2017 roku. Wykorzystuje ona metodę aktor- krytyk, posługując się dwoma sieciami neuronowymi. Aktor jest reprezentowany przez sieć neuronową, która wykonuje zadanie klasyfikacji, wybierając, jaką akcję podjąć przy obserwowanym stanie środowiska. Krytyk to druga sieć, o~podobnej strukturze, która uczy się oceniać, czy poszczególne akcje poprawiają, czy pogarszają stan. Krytyk zwraca Q-wartość akcji wybranej w~poprzednim stanie. Na tej podstawie aktor modyfikuje swoją politykę, dbając, by nie przeprowadzić zbyt dużych zmian.

W~metodzie PPO aktualna polityka wykorzystywana jest do przeprowadzenia pewnej liczby doświadczeń (wielkość tę określa się mianem rozmiaru batcha) wymagających interakcji ze środowiskiem, które wykorzystywane są do poprawy polityki. Jest to uczenie on-policy, jako, że zebrane dane są wykorzystywane tylko raz.

Algorytm korzysta z~mechanizmu Uogólnionej Estymacji Przewagi (ang. \textit{Generalized Average Estimation}), aby ustalić, które akcje przyniosły, jakie skutki. Liczona jest ona od ostatnich wykonywanych akcji. Obliczane są kolejno wartości:

\begin{equation}
	\delta = r_t + \gamma V(s_{t+1})m_t - V(s_t)
\end{equation}

\begin{equation}
	gae_t = \delta + \gamma \lambda m_t gae_{t+1}
\end{equation}

\begin{equation}
	R_t(s_t,a_t) = gae_t + V(s_t)
\end{equation}

gdzie:
\begin{itemize}
	\item $t$ - numer kroku analizowanego testu,
	\item $s_t$ - stan w chwili $t$,
	\item $a_t$ - akcja podejmowana w~chwili $t$,
	\item $V(s)$ - wartość stanu $s$,
	\item $r_t$ - nagroda w~chwili $t$.
	\item $m_t$ - maska w~chwili $t$, jeśli w~stan $t$ jest końcowy, to 0, w~przeciwnym wypadku 1.
\end{itemize}

Wystąpiły tutaj dwa hiperparametry algorytmu:
\begin{itemize}
	\item $\gamma$ - parametr dyskontowy, standardowo 0.99,
	\item $\lambda$ - parametr wygładzający, standardowo 0.95.
\end{itemize}

Wyliczane jest ratio, które mówi, jak bardzo polityka uległa zmianie.
\begin{equation}
	ratio = \pi_{new} / \pi_{old}
\end{equation}
Polityka jest oceniana przez aktora i~krytyka. Ocena krytyka to błąd średniokwadratowy pomiędzy wartością zwracaną a oszacowaniem krytyka.
\begin{equation}
	critic\_loss = (R - V(s))^2
\end{equation}

Natomiast błąd aktora wyraża się wzorem:
\begin{equation}
	actor\_loss = min(ratio * advantage, clip(ratio, 1-\varepsilon,1+\varepsilon)* advantage)
\end{equation}

$\varepsilon$ to parametr obcinania, który zapewnia, że jednorazowo zmienimy politykę o~co najwyżej $\varepsilon\%$. Domyślna wartość tego parametru wynosi 0.2.

Ostatecznie ocena polityki wyraża się wzorem:
\begin{equation}
	total\_loss = critic\_loss * critic\_discount + actor\_loss - entropy
\end{equation}

$critic\_discount$ to współczynnik mający na celu zrównoważenie wpływu na wynik oceny aktora i~oceny krytyka. Entropia to natomiast hiperparametr metody, który domyślnie przyjmuje wartość 0.005. 

Na podstawie powyższych funkcji oceny używane sieci uczone są przy użyciu metody stochastycznego przyrostu gradientu. Sieć jest uczona na danych z~pojedynczego batcha przez pewną liczbę epok, będącą hiperparametrem metody.

Opis działania metody PPO został oparty na artykułach C. Trivediego \cite{ppo1},\cite{ppo2}.

% W rozdziałach o PPO i SAC chcemy teoretycznego wprowadzenia, co to za algorytmy i jak działają. Myślę, że nie warto skupiać się na bardzo low-levelowej matmie, raczej skupmy się na charakterze obu tych metod, a taki matematyczny background wprowadzić bardzo lekko. Niżej wrzucam link do parametrów, które możemy konfigurować w kontekście obu algorytmów - Trainer-specific Configurations. Podczas opisywania skup się na nich. Warto opisać wszystkie hyperparameters, a są też network_settings, opisujące sieci które są tworzone, więc na ten temat trzeba napisać w subsection Sieci neuronowe.
%https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Configuration-File.md#trainer-specific-configurations

\subsection{SAC}

Soft Actor-Critic, czyli SAC to też stosunkowo nowa metoda do rozwiązywania tego typu problemów. W~przeciwieństwie do PPO jest to metoda off-policy, czyli wyniki uzyskane przy użyciu jednej polityki są wykorzystywane do oceny następnych polityk.

Algorytm ten modyfikuje funkcję celu, aby maksymalizować nie tylko nagrody, ale również entropię polityki, aby wspierać eksplorację przestrzeni stanów. Funkcja ta jest zdefiniowana następująco:

\begin{equation}
	J(\pi) = \sum_{t=0}^T \mathbb{E}_{(s_t,a_t) \sim \rho_\pi} [r(s_t,a_t) + \alpha H(\pi(\cdot |s_t))]
\end{equation}

$\alpha$ to parametr, który określa wagę entropii.

SAC używa trzech sieci neuronowych:
\begin{itemize}
	\item określającej wartość stanu V z~parametrem $\psi$,
	\item określającej funkcję Q z~parametrem $\theta$,
	\item określającej funkcję polityki z~parametrem $\phi$.
\end{itemize}

Sieć V jest uczona tak, aby zminimalizować błąd  kwadratowy pomiędzy predykcją wartości zwracanej przez sieć a wartością oczekiwaną sumy funkcji Q i~entropii:

\begin{equation}
J_V(\psi) = \mathbb{E}_{s_t \sim D}\Big[ \frac{1}{2}\big(V_\psi(s_t) - \mathbb{E}_{a_t ~\pi_\phi} [Q_\theta(s_t,a_t) - log \pi_\phi (a_t|s_t)] \big)^2\Big]
\end{equation}

Sieć Q minimalizuje błąd średniokwadratowy pomiędzy predykcją funkcji Q a natychmiastową nagrodą zsumowaną ze zdyskontowaną predykcją wartości następnego stanu.

\begin{equation}
	J_Q(\theta) = \mathbb(E)_{(s_t,a_t) \sim D}\Big[
		\frac{1}{2} \big(
			Q_\theta(s_t,a_t) - r(s_t,a_t) - \gamma \mathbb{E}_{s_{t+1 \sim p}}[V_{\bar{\psi}}(s_{t+1})]
		\big)
	\Big]
\end{equation}

Sieć $\pi$ ma za zadanie minimalizować różnicę pomiędzy wartością zwracaną przez sieć  a~wartością $\frac{exp(Q_\theta(s_t,\cdot))}{Z_\theta(s_t)}$, gdzie Z to funkcja normalizująca. Matematyczny opis tej zależności wykorzystuje dywergencję Kullbacka-Leiblera i~można go przedstawić przy użyciu wzoru:

\begin{equation}
	J_\pi(\phi) = \mathbb(E)_{s_t \sim D}\Big[ D_{KL} \big(\pi_\phi(\cdot|s_t) || \frac{exp(Q_\theta(s_t,\cdot))}{Z_\theta(s_t)}  \big) \Big]
\end{equation}

Przy wyliczaniu tej wartości używana jest też pewna technika reparemetryzacji.



Schemat algorytmu przedstawiono na Listingu \ref{alg:sac}.
\begin{algorithm}[h!]
	\caption{SAC}
	\label{alg:sac}
	\begin{algorithmic}[1]
		\STATE $\psi, \bar{\psi}, \theta,\phi - dane parametry$
		\FOR {$i=0,\dots,max\_iteration$}
			\FOR {ruch środowiska}
				\STATE $a_t \sim \pi_{\phi}(a_t|s_t)$
				\STATE $_{t+1} \sim p(s_{t+1}|s_t,a_t)$
				\STATE $D \gets D \cup \{(s_t,a_t,r(s_t,a_t),s_{t+1})\}$
			\ENDFOR
		\ENDFOR
		\STATE Popraw parametry sieci przy użyciu gradientów poszczególnych funkcji dopasowania.
	\end{algorithmic}
\end{algorithm}

Opis algorytmu sporządzono w oparciu o~artykuły \cite{kumar},\cite{nieznany}.

W~przypadku algorytmu SAC badano wpływ na wyniki początkowego ustawienia wartości parametru $\alpha$ związanego z~entropią. Zbadano również działanie parametru opisującego liczbę doświadczeń zbieranych do bufora przed aktualizacją polityki. Domyślnie wartość ta wynosi 0, ale powszechne jest wykorzystywaniu w~tym miejscu 1000 - 10000 doświadczeń.

W~przypadku obu algorytmów przetestowano różne rozmiary batcha, parametry tempa uczenia sieci neuronowych oraz liczby ich warstw.

\section{Eksperymenty}
% Trzeba opisać czynniki które uwzględniamy przy uczeniu: checkpointy, liczba raycastów, wartość wypłat itp.


\section{Wnioski}


\begin{thebibliography}{20}
	\bibitem[1]{kumar}Kumar V., \textit{Soft Actor-Critic Demystified}, \url{https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665}. 2019.
	\bibitem[2]{ppo1} Trivedi C., \textit{Proximal Policy Optimization Tutorial (Part 1/2: Actor-Critic Method)}, \url{https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6}, 2019.
	\bibitem[3]{ppo2} Trivedi C., \textit{Proximal Policy Optimization Tutorial (Part 2/2: GAE and PPO loss)}, \url{https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8}, 2019.
	\bibitem[4]{nieznany} \textit{SAC (soft actor-critic) algorithm for reinforcement learning}, \url{https://www.programmersought.com/article/77705189397/}
\end{thebibliography}



\end{document}
