\documentclass[a4paper,12pt]{article}
\usepackage{titling}
\usepackage[OT4]{fontenc}
\usepackage[english,polish]{babel}
\usepackage{amsmath, amsfonts, amsthm, latexsym}

% Margins in document
%\usepackage[left=2.5cm, right=2.5cm, top=3.5cm]{geometry}

% Indentation at the beginning of chapters/sections
\usepackage{indentfirst}

% Ceiling functions
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% Avoid  colons before tables' empty captions and change caption
\usepackage{caption}
\captionsetup[table]{name=Tab.}
\captionsetup[figure]{name=Rys.}

% Don't know why, it starts from 2
\addtocounter{table}{0}

% Rename tables' suffix
\renewcommand{\tablename}{Tab.}

% Graphicx setup
\usepackage{graphicx}
\graphicspath{{graphics/}{../graphics/}}

% No separator between items
\usepackage{enumitem}
\setlist{nolistsep}

% Pagebreak before every \section
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

% Bigger padding in tabulars
\usepackage{array}
\setlength\extrarowheight{3pt}

%\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

% Itemize in tabulars (avoid big margins with minipage)
\newcommand{\tabbeditemize}[1]{
	\begin{minipage}[t]{0.4\textwidth}
		\begin{itemize}[topsep=0mm,partopsep=0mm,leftmargin=4mm]
			#1
		\end{itemize}
\end{minipage}}

% listings
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{pdfpages}
\definecolor{codegreen}{rgb}{0,0.7,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
	language=Python,
	deletekeywords={from},
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=4
}
\makeatletter
\newcommand*{\toccontents}{\@starttoc{tableofcontents}}
\makeatother


% DOCUMENT
\title{Sprawozdanie \\
\Large Zastosowanie uczenia maszynowego do gry wyścigowej}

\author{Mateusz Burczaniuk,\\ Patryk Fijałkowski}
\date{07.12.2020}


% ============================================
% CONTENT ====================================
% ============================================

\begin{document}
	\maketitle
	%\vspace{3cm}
	\tableofcontents

\section{Wprowadzenie}
\subsection{Problem}
Omawiana gra wyścigowa symuluje rywalizację na torze pomiędzy kilkoma samochodami wyścigowymi. Tor, na którym odbywa się rywalizacja jest zamkniętą pętlą otoczoną niezbyt wysokimi ścianami. Poza tym na torze występują także przeszkody. Samochód może poruszać się do przodu, do tyłu, skręcać w~lewo, prawo lub wykorzystać nitro. Zderzenie samochodu gracza ze ścianą lub przeszkodą powoduje natychmiastową śmierć gracza i~koniec gry. Zderzenie dwóch graczy ze sobą powoduje analogiczne skutki (śmierć i~usunięcie z~rozgrywki) dla obu graczy.

Gracz ma zadanie przejechać zadaną trasę jak najszybciej, wyprzedzając swoich rywali. Podczas rywalizacji gracz może znacznie zwiększyć swoją prędkość przy użyciu nitro. Jednak maksymalna dostępna w~danej chwili ilość nitro jest ograniczona, więc gracz nie może używać go bez przerwy. Zużyte nitro stopniowo odnawia się i~po pewnym czasie wraca do pierwotnego poziomu. Może ono odnawiać się nieskończoną liczbę razy.

Wszystkie rozważane pojazdy mają taki same parametry techniczne i~osiągi, w~związku z~czym wygrana lub przegrana w~wyścigu zależy wyłącznie od obranego przez nie sposobu przemieszczania się po torze.

Zatem postawiony problem składa się z~kilku zagadnień - należy zoptymalizować politykę doboru kierunku poruszania się gracza oraz używania nitro tak, aby przejechać całą trasę bez żadnego zderzenia, a~jednocześnie jak najszybciej, pozostawiając rywali za sobą. Zadania te w~pewnym stopniu konfliktują ze sobą, gdyż optymalną strategią w~celu uniknięcia zderzeń jest stanie w~miejscu, natomiast dążenie do uzyskania optymalnego czasu przejazdu powoduje wzrost ryzyka zderzenia z~przeszkodą lub innym pojazdem.

Problem próbowano rozwiązać za pomocą dwóch metod: PPO oraz SAC.

\subsection{PPO}
Proximal Policy Optimization, czyli PPO to nowa metoda, wprowadzona w~2017 roku. Wykorzystuje ona metodę aktor- krytyk, posługując się dwoma sieciami neuronowymi. Aktor jest reprezentowany przez sieć neuronową, która wykonuje zadanie klasyfikacji, wybierając, jaką akcję podjąć przy obserwowanym stanie środowiska. Krytyk to druga sieć, o~podobnej strukturze, która uczy się oceniać, czy poszczególne akcje poprawiają, czy pogarszają stan. Krytyk zwraca Q-wartość akcji wybranej w~poprzednim stanie. Na tej podstawie aktor modyfikuje swoją politykę, dbając, by nie przeprowadzić zbyt dużych zmian.

W~metodzie PPO aktualna polityka wykorzystywana jest do przeprowadzenia pewnej liczby doświadczeń (wielkość tę określa się mianem rozmiaru batcha) wymagających interakcji ze środowiskiem, które wykorzystywane są do poprawy polityki. Jest to uczenie on-policy, jako, że zebrane dane są wykorzystywane tylko raz.

Algorytm korzysta z~mechanizmu Uogólnionej Estymacji Przewagi (ang. \textit{Generalized Average Estimation}), aby ustalić, które akcje przyniosły, jakie skutki. Liczona jest ona od ostatnich wykonywanych akcji. Obliczane są kolejno wartości:

\begin{equation}
	\delta = r_t + \gamma V(s_{t+1})m_t - V(s_t)
\end{equation}

\begin{equation}
	gae_t = \delta + \gamma \lambda m_t gae_{t+1}
\end{equation}

\begin{equation}
	R_t(s_t,a_t) = gae_t + V(s_t)
\end{equation}

gdzie:
\begin{itemize}
	\item $t$ - numer kroku analizowanego testu,
	\item $s_t$ - stan w chwili $t$,
	\item $a_t$ - akcja podejmowana w~chwili $t$,
	\item $V(s)$ - wartość stanu $s$,
	\item $r_t$ - nagroda w~chwili $t$.
	\item $m_t$ - maska w~chwili $t$, jeśli w~stan $t$ jest końcowy, to 0, w~przeciwnym wypadku 1.
\end{itemize}

Wystąpiły tutaj dwa hiperparametry algorytmu:
\begin{itemize}
	\item $\gamma$ - parametr dyskontowy, standardowo 0.99,
	\item $\lambda$ - parametr wygładzający, standardowo 0.95.
\end{itemize}

Wyliczane jest ratio, które mówi, jak bardzo polityka uległa zmianie.
\begin{equation}
	ratio = \pi_{new} / \pi_{old}
\end{equation}
Polityka jest oceniana przez aktora i~krytyka. Ocena krytyka to błąd średniokwadratowy pomiędzy wartością zwracaną a oszacowaniem krytyka.
\begin{equation}
	critic\_loss = (R - V(s))^2
\end{equation}

Natomiast błąd aktora wyraża się wzorem:
\begin{equation}
	actor\_loss = min(ratio * advantage, clip(ratio, 1-\varepsilon,1+\varepsilon)* advantage)
\end{equation}

$\varepsilon$ to parametr obcinania, który zapewnia, że jednorazowo zmienimy politykę o~co najwyżej $\varepsilon\%$. Domyślna wartość tego parametru wynosi 0.2.

Ostatecznie ocena polityki wyraża się wzorem:
\begin{equation}
	total\_loss = critic\_loss * critic\_discount + actor\_loss - entropy
\end{equation}

$critic\_discount$ to współczynnik mający na celu zrównoważenie wpływu na wynik oceny aktora i~oceny krytyka. Entropia to natomiast hiperparametr metody, który domyślnie przyjmuje wartość 0.005. 

Na podstawie powyższych funkcji oceny używane sieci uczone są przy użyciu metody stochastycznego przyrostu gradientu. Sieć jest uczona na danych z~pojedynczego batcha przez pewną liczbę epok, będącą hiperparametrem metody.

Opis działania metody PPO został oparty na artykułach C. Trivediego \cite{ppo1},\cite{ppo2}.

% W rozdziałach o PPO i SAC chcemy teoretycznego wprowadzenia, co to za algorytmy i jak działają. Myślę, że nie warto skupiać się na bardzo low-levelowej matmie, raczej skupmy się na charakterze obu tych metod, a taki matematyczny background wprowadzić bardzo lekko. Niżej wrzucam link do parametrów, które możemy konfigurować w kontekście obu algorytmów - Trainer-specific Configurations. Podczas opisywania skup się na nich. Warto opisać wszystkie hyperparameters, a są też network_settings, opisujące sieci które są tworzone, więc na ten temat trzeba napisać w subsection Sieci neuronowe.
%https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Configuration-File.md#trainer-specific-configurations

\subsection{SAC}



W~przypadku obu algorytmów zbadane zostaną rozmiar batcha, parametr tempa uczenia sieci neuronowych oraz liczba ich warstw.

\section{Eksperymenty}
% Trzeba opisać czynniki które uwzględniamy przy uczeniu: checkpointy, liczba raycastów, wartość wypłat itp.


\section{Wnioski}


\begin{thebibliography}{20}
	\bibitem[1]{ppo1} Trivedi C., \textit{Proximal Policy Optimization Tutorial (Part 1/2: Actor-Critic Method)}, \url{https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6}, 2019.
	\bibitem[2]{ppo2} Trivedi C., \textit{Proximal Policy Optimization Tutorial (Part 2/2: GAE and PPO loss)}, \url{https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8}, 2019.
	%\bibitem[1]{kret} Krętowska M., \textit{Sztuczne sieci neuronowe. Wykład 12: Sieci samoorganizujące się typu Hebba}, \url{http://aragorn.pb.bialystok.pl/~gkret/SSN/SSN_w12.PDF}
	%\bibitem[2]{similar} N.M. Nasrabadi, W. Li, \textit{Object recognition by a Hopfield neural network}, IEEE Transactions on Systems, Man, and Cybernetics, vol. 21, no. 6, pp. 1523-1535, 1991.
	%\bibitem[3]{capacity} Y. Abu-Mostafa and J. St. Jacques, \textit{Information capacity of the Hopfield model}, in IEEE Transactions on Information Theory, vol. 31, no. 4, pp. 461-464, 1985.
\end{thebibliography}



\end{document}
